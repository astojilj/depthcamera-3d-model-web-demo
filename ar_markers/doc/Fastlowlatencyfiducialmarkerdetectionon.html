<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=iMc4-4exjgqA-toZgqqJBf35jAjY-31lSKWZb9Fa77M');.lst-kix_u3iog4hz9w7y-7>li:before{content:"\0025cb  "}.lst-kix_u3iog4hz9w7y-4>li:before{content:"\0025cb  "}.lst-kix_u3iog4hz9w7y-8>li:before{content:"\0025a0  "}.lst-kix_u3iog4hz9w7y-5>li:before{content:"\0025a0  "}.lst-kix_u3iog4hz9w7y-6>li:before{content:"\0025cf  "}ul.lst-kix_mkjo0yq6ldhg-4{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-3{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-2{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-1{list-style-type:none}.lst-kix_u3iog4hz9w7y-1>li:before{content:"\0025cb  "}ul.lst-kix_mkjo0yq6ldhg-8{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-7{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-6{list-style-type:none}ul.lst-kix_mkjo0yq6ldhg-5{list-style-type:none}.lst-kix_u3iog4hz9w7y-0>li:before{content:"\0025cf  "}.lst-kix_u3iog4hz9w7y-3>li:before{content:"\0025cf  "}ul.lst-kix_mkjo0yq6ldhg-0{list-style-type:none}.lst-kix_mkjo0yq6ldhg-7>li:before{content:"\0025cb  "}.lst-kix_u3iog4hz9w7y-2>li:before{content:"\0025a0  "}.lst-kix_mkjo0yq6ldhg-6>li:before{content:"\0025cf  "}.lst-kix_mkjo0yq6ldhg-4>li:before{content:"\0025cb  "}.lst-kix_mkjo0yq6ldhg-8>li:before{content:"\0025a0  "}ul.lst-kix_u3iog4hz9w7y-8{list-style-type:none}.lst-kix_mkjo0yq6ldhg-5>li:before{content:"\0025a0  "}ul.lst-kix_u3iog4hz9w7y-7{list-style-type:none}ul.lst-kix_u3iog4hz9w7y-6{list-style-type:none}.lst-kix_mkjo0yq6ldhg-2>li:before{content:"\0025a0  "}ul.lst-kix_u3iog4hz9w7y-5{list-style-type:none}ul.lst-kix_u3iog4hz9w7y-4{list-style-type:none}ul.lst-kix_u3iog4hz9w7y-3{list-style-type:none}ul.lst-kix_u3iog4hz9w7y-2{list-style-type:none}ul.lst-kix_u3iog4hz9w7y-1{list-style-type:none}ul.lst-kix_u3iog4hz9w7y-0{list-style-type:none}.lst-kix_mkjo0yq6ldhg-3>li:before{content:"\0025cf  "}.lst-kix_mkjo0yq6ldhg-0>li:before{content:"\0025a0  "}.lst-kix_mkjo0yq6ldhg-1>li:before{content:"\0025cb  "}ol{margin:0;padding:0}table td,table th{padding:0}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c13{padding-top:0pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c32{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c18{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c6{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c20{color:#434343;text-decoration:none;vertical-align:baseline;font-size:14pt;font-style:normal}.c3{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none;font-size:12pt}.c23{padding-top:0pt;padding-bottom:3pt;line-height:1.0;page-break-after:avoid;text-align:left}.c4{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c24{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;text-align:left}.c8{color:#000000;text-decoration:none;vertical-align:baseline;font-size:20pt;font-style:normal}.c29{padding-top:20pt;padding-bottom:6pt;line-height:1.0;page-break-after:avoid;text-align:left}.c9{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.2;text-align:left}.c10{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c25{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c0{font-weight:400;font-family:"Open Sans"}.c21{font-weight:400;font-family:"Arial"}.c7{color:inherit;text-decoration:inherit}.c28{padding:0;margin:0}.c30{width:33%;height:1px}.c16{margin-left:36pt;padding-left:0pt}.c31{background-color:#000000;color:#ffd966}.c12{font-weight:700;font-family:"Open Sans"}.c19{font-size:10pt}.c5{font-size:12pt}.c1{font-style:italic}.c26{background-color:#fff2cc}.c17{font-size:16pt}.c27{font-size:26pt}.c11{height:11pt}.c22{font-size:14pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c25"><div><p class="c2 c11"><span class="c4 c21"></span></p></div><p class="c23 title" id="h.4v9lfldn86u"><span class="c10 c0 c27">Fast, low latency fiducial marker detection on GPU in browser (WebGL)</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c15"><span class="c9 c0 c22"><a class="c7" href="mailto:aleksandar.stojiljkovic@intel.com">aleksandar.stojiljkovic@intel.com</a></span></p><p class="c15 c11"><span class="c10 c0 c22"></span></p><p class="c14"><span class="c0 c22 c26">Note: this article is work in progress and serves here only to document the code - the final version is expected to be published on 01.org in following days.</span></p><h1 class="c29" id="h.csfaz1ff6j0f"><span class="c0">1. </span><span class="c8 c0">Introduction</span></h1><p class="c15 c11"><span class="c10 c0 c22"></span></p><p class="c15"><span class="c0 c5">Work on this started as part of 3D model scanning </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo&amp;sa=D&amp;ust=1526980286264000">implementation </a></span><span class="c0 c5">using depth camera capture in Chrome web browser. There, the plan is to use </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Iterative_closest_point&amp;sa=D&amp;ust=1526980286264000">ICP</a></span><span class="c0 c5">&nbsp;algorithm</span><span class="c0 c5">&nbsp;for camera movement estimation, but implementing ICP using WebGL only is a complex task. So, </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Fiducial_marker%23Augmented_reality&amp;sa=D&amp;ust=1526980286264000">fiducial AR markers</a></span><span class="c10 c0 c5">&nbsp;started to look like an easier and a sooner available alternative. Additional benefit that the AR markers would bring is marking a cut off plane separating scanned 3D object from the environment.</span></p><p class="c15 c11"><span class="c10 c0 c5"></span></p><p class="c15"><span class="c0 c5">Around the same time other ideas came up to use webcam and AR markers - &nbsp;webcam could be used to augment environment, or to compute motion data from webcam video frame to add more immersive VR experience or improve accessibility by calculating viewing position and direction. At first, I was skeptical about user input based on webcam capture, given that 30 FPS webcam introduces 33 ms latency, plus there is some latency added by a capture pipeline and . . . All together, it seemed like it too much of a latency for VR.<br><br>Then again, if reusing current, OpenCV based approaches, part of the processing would be done in JavaScript and reading back the pixels from HTML5 canvas (2D or WebGL context ) would increase latency. So, detection of markers would need to be done in WebGL shaders code, without reading back the pixels from GPU and the detection results should be available with low latency.<br><br>Using slow motion capture on my iPhone, measured this latency and it is </span><span class="c3 c0"><a class="c7" href="#h.fkekl6kcowp">better than initially assumed</a></span><span class="c0 c5">. In short, it works so that the video frame is uploaded to GPU texture, shaders detect the markers position within the same rendering frame, with minor additional latency</span><sup class="c0 c5"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c10 c0 c5">&nbsp;compared to latency when only webcam video frame is displayed. Video texture and the detected markers then get rendered. There is no readback from GPU textures to CPU side - all the recognition processing happens on GPU (render-to-texture shader passes) and the result is consumed in render-to-screen shader pass. &nbsp;Demo code also provides the readback implementation, if needed.</span></p><p class="c15 c11"><span class="c10 c0 c5"></span></p><p class="c15"><span class="c10 c0 c5">The article here is explaining:</span></p><ul class="c28 lst-kix_u3iog4hz9w7y-0 start"><li class="c15 c16"><span class="c10 c0 c5">Current status of the prototype code and the constraints.</span></li><li class="c15 c16"><span class="c10 c0 c5">Algorithm steps, so that one could easier modify it for different type of AR markers or purpose.</span></li><li class="c15 c16"><span class="c10 c0 c5">Latency measurement using slow motion camera and EXT_disjoint_timer_query</span></li></ul><p class="c15 c11"><span class="c10 c0 c5"></span></p><p class="c15"><span class="c0 c5">The code is not implementing </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://docs.opencv.org/3.3.0/dc/d2c/tutorial_real_time_pose.html&amp;sa=D&amp;ust=1526980286266000">camera pose estimation</a></span><span class="c0 c5">&nbsp;based on identified marker position - </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://docs.opencv.org/3.4.0/d9/d0c/group__calib3d.html%23gae5af86788e99948d40b39a03f6acf623&amp;sa=D&amp;ust=1526980286266000">solving P3P</a></span><span class="c0 c5">&nbsp;in WebGL shader could be a nice topic for the next article, but for the use case mentioned here - 3D model scanning using depth capture camera</span><sup class="c0 c5"><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c0 c5">&nbsp;- it is not required: we get marker square corners 3D position from depth stream, using the approach described in </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://www.w3.org/TR/mediacapture-depth/%23synchronizing-depth-and-color-video-rendering&amp;sa=D&amp;ust=1526980286267000">Media Capture, Depth Stream Extension</a></span><span class="c10 c0 c5">&nbsp;W3C API.</span></p><h1 class="c18" id="h.2o0wsym4lm71"><span class="c0">2. Current s</span><span class="c8 c0">tatus</span></h1><h2 class="c6" id="h.79ywo03nuw2y"><span class="c10 c0 c17">2.1 Implementation supports 3x3 AR codes.</span></h2><p class="c2"><span class="c0 c5">3x3 marker can uniquely identify 64 different codes. For the convenience, here is a </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/tree/master/ar_markers/3x3&amp;sa=D&amp;ust=1526980286267000">link to print-ready images </a></span><span class="c0 c5">containing all of the codes, from 0 to 63. For the generators, check the links available at </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/artoolkit/artoolkit5/issues/264&amp;sa=D&amp;ust=1526980286268000">ARToolKit</a></span><span class="c0 c5">. Modifying the algorithm for other types of markers is explained at algorithm step </span><span class="c3 c0"><a class="c7" href="#h.y0xfjuvtvrsn">5. Identify markers</a></span><span class="c10 c0 c5">.</span></p><p class="c2 c11"><span class="c10 c0 c22"></span></p><h2 class="c6" id="h.r4voon8fvhdb"><span class="c0">2.2 </span><span class="c0">It is fast and can detect multiple markers simultaneously</span></h2><p class="c2"><span class="c10 c0 c5">This (regular speed) video shows browser canvas that is rendering capture from integrated 30Hz laptop webcam:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 350.67px;"><img alt="" src="images/image4.gif" style="width: 624.00px; height: 350.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0 c5">Notice that the marker square at the bottom right of the paper, contains the label &ldquo;NOT VALID&rdquo; and it is not recognized. Other markers, with valid code and layout, are recognized and the code, yellowish numbers near markers&rsquo; corners </span><span class="c0 c5 c1">0</span><sup class="c0 c5"><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c10 c0 c5">&nbsp;are rendered.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">Next is a slow motion video, captured by iPhone at 240Hz - so, if my calculation is right, it should be 8 times slower than regular speed. </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://software.intel.com/en-us/realsense/sr300&amp;sa=D&amp;ust=1526980286269000">USB camera</a></span><span class="c0 c5">&nbsp;samples at 60Hz and the </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://www.asus.com/us/Laptops/ROG-GL702VM-7th-Gen-Intel-Core/&amp;sa=D&amp;ust=1526980286269000">laptop</a></span><span class="c0 c5">&rsquo;s </span><span class="c0 c5">display refresh rate is 75Hz.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 350.67px;"><img alt="" src="images/image3.gif" style="width: 624.00px; height: 350.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">As mentioned, when the marker coordinates and the code are recognized, yellow number is visible on laptop screen, next to the marker </span><span class="c3 c0"><a class="c7" href="#h.y0xfjuvtvrsn">corner </a></span><span class="c3 c0 c1"><a class="c7" href="#h.y0xfjuvtvrsn">0</a></span><span class="c10 c0 c5">.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">Notice that all of the markers, from the paper on the left, are detected and that the code </span><span class="c12 c5 c31">0</span><span class="c0 c5">&nbsp;from the marker in my hand, that is on the right and is moving very fast, is also detected, except in cases when there is a significant motion blur. The same video, in regular speed is available for </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/videos_for_documentation/ar_markers/doc/IMG_0929.MOV&amp;sa=D&amp;ust=1526980286270000">download here</a></span><span class="c10 c0 c5">, if you&rsquo;d like to check it further.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><h2 class="c6" id="h.jdg4s1lp498q"><span class="c0">2.3 </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://docs.opencv.org/3.1.0/d5/dae/tutorial_aruco_detection.html&amp;sa=D&amp;ust=1526980286271000">Adaptive thresholding</a></span><span class="c10 c0 c17">&nbsp;is not yet implemented: demo requires controlled lighting</span></h2><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0 c5">This should be the very first step in </span><span class="c3 c0"><a class="c7" href="#h.vyknjgh7bumr">the algorithm</a></span><span class="c0 c5">&nbsp;and it would make the algorithm work in different lighting conditions. Deliberately, I left it for later, to tweak it in the </span><span class="c0 c3"><a class="c7" href="#h.nnp4xgal5o3g">first shader pass</a></span><span class="c10 c0 c5">, without compromising the latency. So, for now this works only in reasonably lit room when there is no direct light in the same direction from camera to marker.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><h2 class="c6" id="h.3goazrpevasy"><span class="c0">2.4 This is a f</span><span class="c10 c0 c17">irst step, prototype code</span></h2><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">This is a prototype code, the first step in implementing robust and performant AR label detection. Work on it is going to continue while integrating it to 3D model scanning, initially and plan to explore other use cases, as </span><span class="c3 c0"><a class="c7" href="#h.csfaz1ff6j0f">mentioned</a></span><span class="c0 c5">.</span></p><h2 class="c6" id="h.7v8t43eg0lai"><span class="c0">2.5 </span><span class="c10 c0 c17">Browser requirements</span></h2><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">It is verified to run on Chrome, version 64 or later</span><span class="c0 c5">, and o</span><span class="c10 c0 c5">n Firefox, version 60, when browsers were running on on Windows 10, Ubuntu 16.04 or macOS Sierra.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">For Firefox, since the </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L90&amp;sa=D&amp;ust=1526980286273000">demo.html</a></span><span class="c0 c5">&nbsp;code uses </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://html.spec.whatwg.org/multipage/custom-elements.html&amp;sa=D&amp;ust=1526980286273000">custom elements</a></span><span class="c0 c5">, it is required to open about:config page in Firefox and set </span><span class="c0 c5 c1">dom.webcomponents.customelements.enabled</span><span class="c0 c5">&nbsp;and </span><span class="c0 c5 c1">dom.webcomponents.shadowdom.enabled</span><span class="c10 c0 c5">&nbsp;to true. This dependency is related only to page layout code and it can be easily removed, to void the need for dealing with about:config.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><h1 class="c18" id="h.1doopuverw4"><span class="c0">3. </span><span class="c0 c8">Algorithm and the code</span></h1><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">The steps of algorithm and the rendering it to screen is a cascade of rendering passes, where rendering passes are rendering to texture or to screen, and each render pass output texture is input for, one or more, following rendering passes. So, referring here to algorithm steps as </span><span class="c0 c1">passes</span><span class="c0">&nbsp;and the order corresponds to execution order. In code, for each of the </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L1081&amp;sa=D&amp;ust=1526980286275000">passes</a></span><span class="c0">, output is defined by property </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L1083&amp;sa=D&amp;ust=1526980286275000">framebuffer</a></span><span class="c4 c0">&nbsp;- when it is null, we render to screen.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 210.50px; height: 195.86px;"><img alt="" src="images/image5.png" style="width: 210.50px; height: 195.86px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">Algorithm focuses on identifying inner square, highlighted on the picture to the left. After identifying straight edges and four pixels of the square, in step 5 it samples cells inside and detects the code.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2 c11"><span class="c4 c0"></span></p><h2 class="c6" id="h.qnz5cdqebqdu"><span class="c0">3.1 Pass #0: Threshold and edges code</span></h2><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">As mentioned, </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://docs.opencv.org/3.1.0/d5/dae/tutorial_aruco_detection.html&amp;sa=D&amp;ust=1526980286276000">adaptive thresholding</a></span><span class="c4 c0">&nbsp;is not yet implemented. For now, we use simple constant threshold to tell apart black from white pixels - sometimes it works, sometimes it doesn&rsquo;t, depending on the room lighting. It works in constrained conditions, then it is able to tell apart black from white pixels and from there to identify the edges. By &ldquo;edge pixels&rdquo; we will refer to black pixels having white pixels next to them.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 575.50px; height: 354.76px;"><img alt="" src="images/image6.png" style="width: 575.50px; height: 354.76px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c4 c0">This step sets the foundation for processing in following steps - it encodes edge pixels: for each of the edge pixels, enumerates neighboring pixels in counterclockwise order (so, it&rsquo;s eight neighbors) and, in that order of enumeration, identifies a sequence of black pixels. For the sequence, or we could call it arc of black pixels, values for start-of-black (S) and end-of-black (E) are calculated, as described by this diagram:</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c32"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 479.00px; height: 181.72px;"><img alt="" src="images/image7.png" style="width: 479.00px; height: 181.72px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c4 c0">The step also calculates the direction between end-of-black and start-of-black - it is marked as B, bisector of the white angle on the diagram above. P, on the diagram, refers to the pixel currently being analyzed and written to framebuffer.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">Following </span><span class="c0 c1">S</span><span class="c0">&nbsp;(start) and </span><span class="c0 c1">E</span><span class="c0">&nbsp;(end) calculated in this step, in later steps we can enumerate all of the edge pixels on inner-edge of square. &nbsp;Start from point </span><span class="c0 c1">P</span><span class="c0">, and following </span><span class="c0 c1">S</span><span class="c0">&nbsp;links, come back to P. There is one specific case, where pixels get skipped if following </span><span class="c0 c1">S</span><span class="c0">-chain - in the diagram above, for both </span><span class="c0 c1">P</span><span class="c0">&nbsp;and </span><span class="c0 c1">0</span><span class="c12">&nbsp;</span><span class="c0">pixels, </span><span class="c0 c1">S</span><span class="c0">&nbsp;points to pixel </span><span class="c0 c1">7</span><span class="c12">. </span><span class="c0">In this case, if we follow S links from </span><span class="c0 c1">4</span><span class="c0">, it would go like </span><span class="c0 c1">4</span><span class="c0">-&gt;</span><span class="c0 c1">P</span><span class="c0">-&gt;</span><span class="c0 c1">7</span><span class="c0">&nbsp;and </span><span class="c0 c1">0</span><span class="c0">&nbsp;would get skipped. In any case, if we start from 0 and follow </span><span class="c0 c1">S</span><span class="c4 c0">&nbsp;links, it&rsquo;s neighboring pixels get reached so we handle this case, too. </span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">Something like this, following S-links, to verify connection between two corners, the algorithm actually does, though it makes it in larger steps, e.g. sample edge pixel on distance of eight S-links in </span><span class="c9 c0"><a class="c7" href="#h.bcbrbu30lgb6">pass 4</a></span><span class="c4 c0">.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">Bisector information is used to identify straight lines - following </span><span class="c0 c1">S</span><span class="c0">&nbsp;links, the </span><span class="c9 c0"><a class="c7" href="#h.d7lvy87y626">next step</a></span><span class="c0">&nbsp;</span><span class="c0">checks the maximal difference in </span><span class="c0 c1">B</span><span class="c4 c0">&nbsp;values, in multiple consecutive pixels.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">Work on calculating </span><span class="c0 c1">S, E</span><span class="c0">&nbsp;and </span><span class="c0 c1">B </span><span class="c0">is done</span><span class="c0">&nbsp;</span><span class="c0">outside shader (so it doesn&rsquo;t repeat on every frame), in </span><span class="c0 c1 c9"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L317&amp;sa=D&amp;ust=1526980286280000">calculateEdgeCodeToGradientDirectionMap </a></span><span class="c0">function: pre-calculated data for all of the potential neighbor values is supplied as uniform array to the shader, the shader </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L468&amp;sa=D&amp;ust=1526980286280000">samples neighbors </a></span><span class="c9 c0 c1"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L468&amp;sa=D&amp;ust=1526980286281000">s0 </a></span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L468&amp;sa=D&amp;ust=1526980286281000">to </a></span><span class="c9 c0 c1"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L468&amp;sa=D&amp;ust=1526980286281000">s7</a></span><span class="c4 c0">, forms a lookup into the precalculated array and writes vec4(B/8.0, S/8.0, E/8.0, 0.0) to framebuffer - to texture color-attached to framebuffer which is going to be input for the next step.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">Following picture shows the </span><span class="c0 c1">S, E</span><span class="c0">&nbsp;and </span><span class="c0 c1">B</span><span class="c4 c0">&nbsp;values for different pixels. </span></p><p class="c2"><span class="c0">&nbsp;</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 649.68px; height: 381.53px;"><img alt="" src="images/image8.png" style="width: 649.68px; height: 381.53px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c11"><span class="c4 c0"></span></p><h2 class="c6" id="h.xjtfvcmcgkoz"><span class="c10 c0 c17">3.2 Pass #1: Corner candidates from edges codes</span></h2><p class="c2 c11"><span class="c0 c4"></span></p><p class="c2"><span class="c0">For each of the edge pixels, this </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L598&amp;sa=D&amp;ust=1526980286282000">step averages</a></span><span class="c0">&nbsp;bisector </span><span class="c0 c1">(B)</span><span class="c0">&nbsp;direction for eight consecutive edge pixels in both, </span><span class="c0 c1">S </span><span class="c0">and</span><span class="c0 c1">&nbsp;E,</span><span class="c0">&nbsp;direction. For every pixel, it also checks the maximal variation in this value. Based on that, the shader is able to estimate </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L618&amp;sa=D&amp;ust=1526980286283000">if the pixel is on straight line</a></span><span class="c4 c0">&nbsp;- if there are no significant changes in direction in 16 edge pixels around, or when the pixel is on edge that is not straight.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">A special case when 8 pixels in S (start) direction form straight line and the 8 pixels in E (end) direction, form it too, but those straight lines make an angle, is when the pixel is corner. Here, we </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L629&amp;sa=D&amp;ust=1526980286284000">discard outer edge corners</a></span><span class="c4 c0">&nbsp;from further analysis.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">Since we sampled, as far as eight edge pixels in </span><span class="c0 c1">S </span><span class="c0">direction, we can </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L643&amp;sa=D&amp;ust=1526980286285000">store the offset to eighth edge pixel in output</a></span><span class="c0">, so that later we can quickly advance when traversing through edge pixels without additional sampling.</span><span class="c0">&nbsp;</span></p><p class="c2 c11"><span class="c4 c0"></span></p><h2 class="c6" id="h.tufn8i34ep2l"><span class="c10 c0 c17">3.3 Pass #2: Refine neighbor corners</span></h2><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">There could be several corner candidates nearby, on a pixel or two distance, as the experience proved so far, and we need to select only one. This step, based on the data passed by previous step, discards any corner candidate that </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L663&amp;sa=D&amp;ust=1526980286286000">has more distinguished corner candidate</a></span><span class="c4 c0">&nbsp;on sampling distance 2.</span></p><h2 class="c6" id="h.u2qek7dslme1"><span class="c10 c0 c17">3.4 Pass #3: Reduce corners texture dimensions, divide both by 5</span></h2><p class="c2"><span class="c0">So far, the passes have been operating on full resolution (960x540, at the time of writing this document). For the client of this computing, it would not be very efficient to sample all 960x540 pixels looking for a detected marker. So, from this step on, following steps are reducing the size of output texture: in this step by dividing width and height by 5, in later steps by further dividing them by 8 and 6, respectively, to final 24x18</span><sup class="c0"><a href="#ftnt4" id="ftnt_ref4">[4]</a></sup><span class="c4 c0">&nbsp;texture. When reusing this script in 3D scanning demo, probably another pass would get appended, to e.g. produce texture 2 by 2 that would only included information for 4 defined marker codes. That should be straightforward, following the code here - for the use case here, we get 24x18 texture carrying the detection results.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">As a result of reduced dimensions, output needs to carry the </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L716&amp;sa=D&amp;ust=1526980286287000">position of the corners</a></span><span class="c4 c0">. &nbsp;</span></p><p class="c2 c11"><span class="c4 c0"></span></p><h2 class="c6" id="h.4cgxtpftl3fn"><span class="c10 c0 c17">3.5 Pass #4: Detect straight line edges between corners</span></h2><p class="c2"><span class="c0">The input for this step is 192x108 texture and the output keeps the same size. Here. for every corner, the shader attempts to traverse through straight-line, 8 step S links until it reaches some other corner. If it reaches the other corner, the output will carry the information of two corners connected via straight inner edge line: </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L834&amp;sa=D&amp;ust=1526980286288000">first corner as RG, the second as BA color components</a></span><span class="c4 c0">.</span></p><h2 class="c6" id="h.e52nqpku6pes"><span class="c10 c0 c17">3.6 Pass #5: identify markers, their edges, corners, orientation and the code</span></h2><p class="c2"><span class="c0">This step reduces the size of output, to </span><span class="c9 c0"><a class="c7" href="#h.u2qek7dslme1">aforementioned 24x18</a></span><span class="c0">&nbsp;texture</span><span class="c0">. For each corner, connected via straight line to another corner, it would try to make four corner hops, following the connection in an attempt to traverse back to itself. If that </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L875&amp;sa=D&amp;ust=1526980286289000">condition is reached</a></span><span class="c4 c0">, the script has identified four connected edges and there is a need to read the inner content.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">This should be made more robust, to avoid false positives, but that work is to be done after </span><span class="c9 c0"><a class="c7" href="#h.qnz5cdqebqdu">adaptive thresholding</a></span><span class="c4 c0">. For now we sample one point per inner cell, total nine of them for 3x3 Hamming code, and first we need to identify the base corner of the marker. Let&rsquo;s then explain how the 3x3 AR code encodes the data.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><h3 class="c24" id="h.pgv54xqekqo9"><span class="c0 c20">3.6.1 3x3 marker data encoding. Base corner.</span></h3><p class="c2"><span class="c4 c0">Base corner is defined by position of two black and one white cell, like on the diagram below. The cells with the numbers, define how the code is computed - if the cell is black, the corresponding number is added to the calculated code. So, the code of the marker on the diagram is then 0 since all the numbers cells are white.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 660.63px; height: 422.73px;"><img alt="" src="images/image1.png" style="width: 660.63px; height: 422.73px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c11"><span class="c4 c0"></span></p><h2 class="c6" id="h.lnhmgchzieyp"><span class="c10 c0 c17">3.7 Rendering</span></h2><p class="c2"><span class="c0">We have two render passes here; line loop that is rendering detected inner square line and the label with marker code, rendered next to </span><span class="c9 c0 c1"><a class="c7" href="#h.e52nqpku6pes">base corner</a></span><span class="c4 c0">.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0">When calculating the position of vertices, both shaders sample the results of previous, recognition passes </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L951&amp;sa=D&amp;ust=1526980286290000">in vertex shader</a></span><span class="c0">. Code label rendering also </span><span class="c9 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L1003&amp;sa=D&amp;ust=1526980286291000">reads the detected code</a></span><span class="c4 c0">&nbsp;and, based on that, renders the code label from the atlas.</span></p><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c4 c0">3.8</span></p><h1 class="c18" id="h.urop97qmeeph"><span class="c8 c0">4. Latency measurement</span></h1><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0 c5">As the </span><span class="c3 c0"><a class="c7" href="#h.csfaz1ff6j0f">introduction</a></span><span class="c0 c5">&nbsp;lays it out, there are different use cases where this could be used; identifying the position in space, user input or fast reading of information. For that, it is interesting to know what is the latency introduced by recognition processing, alone, and what is the total event-to-screen latency, including camera capture, OS and web browsers capture and rendering pipeline latencies, display latency&hellip; The former, we measure using EXT_disjoint_timer_query_webgl2 extension; the latter we try to estimate using slow motion camera.</span></p><p class="c2 c11"><span class="c10 c0 c22"></span></p><h2 class="c6" id="h.fkekl6kcowp"><span class="c0">4.1 </span><span class="c10 c0 c17">EXT_disjoint_timer_query</span></h2><p class="c2 c11"><span class="c4 c0"></span></p><p class="c2"><span class="c0 c5">Using </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://www.khronos.org/registry/webgl/extensions/EXT_disjoint_timer_query_webgl2/&amp;sa=D&amp;ust=1526980286292000">EXT_disjoint_timer_query_webgl2</a></span><span class="c10 c0 c5">&nbsp;extension, it is possible to query GPU for amount of time the recognition related processing takes to complete. The extension is available in Firefox browser, when enabling webgl.enable-privileged-extensions in about:config.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">A little bit about how it works: after </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L1182&amp;sa=D&amp;ust=1526980286293000">enabling</a></span><span class="c0 c5">&nbsp;the extension, </span><span class="c0 c5">WebGLQuery</span><span class="c0 c5">&nbsp;object is </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L1184&amp;sa=D&amp;ust=1526980286293000">created</a></span><span class="c0 c5">&nbsp;and is latter used on every rendering frame,</span><span class="c10 c0 c5">&nbsp;to asynchronously query for the amount of time information.</span></p><p class="c2"><span class="c0 c5">To query it asynchronously, </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L164&amp;sa=D&amp;ust=1526980286294000">beginQuery</a></span><span class="c0 c5">&nbsp;is placed just before issuing drawing calls related to recognition rendering passes. </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L193&amp;sa=D&amp;ust=1526980286294000">After the last</a></span><span class="c0 c5">&nbsp;of the recognition render passes, an endQuery call is coupled with it. We don&rsquo;t attempt to read the query results until the next rendering call, hopefully giving the pending query enough time </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L158&amp;sa=D&amp;ust=1526980286294000">to complete</a></span><span class="c0 c5">. If the result is available, it gets </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L161&amp;sa=D&amp;ust=1526980286295000">printed</a></span><span class="c0 c5">&nbsp;to console and the new query call </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/astojilj/depthcamera-3d-model-web-demo/blob/7b64d5f858cb076b0918cad9a7cd367df0b93e22/ar_markers/demo.html%23L163&amp;sa=D&amp;ust=1526980286295000">could</a></span><span class="c0 c5">&nbsp;</span><span class="c10 c0 c5">begin.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">On </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://www.asus.com/us/Laptops/ROG-GL702VM-7th-Gen-Intel-Core/&amp;sa=D&amp;ust=1526980286296000">Asus ROG-GL702VM</a></span><span class="c10 c0 c5">, recognition related processing takes from 0.2 to 0.3ms to complete on GPU.</span></p><p class="c2"><span class="c0 c5">On </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://support.apple.com/kb/SP704?locale%3Den_US&amp;sa=D&amp;ust=1526980286296000">Mid-2014 MacBook Pro</a></span><span class="c10 c0 c5">&nbsp;with integrated graphics, on macOS, it takes around 5.5ms.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c10 c0 c5">So, this gives us the estimate of the latency introduced by recognition processing and the way to monitor upcoming changes and optimizations.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><h2 class="c6" id="h.qyibpn715o0"><span class="c0">4.2 </span><span class="c10 c0 c17">Slow motion camera</span></h2><p class="c2 c11"><span class="c10 c0 c22"></span></p><p class="c2"><span class="c10 c0 c5">I used iPhone capturing slow motion video at 240 frames per second and then counted number of frames elapsed from the event happening until the event getting displayed on laptop display. iPhone camera is pointed to the same direction as web camera that is capturing 540p video at 60 frames per second - it is pointed towards quickly moving AR label, in my hand, and to the laptop display, detecting the label&rsquo;s marker code and rendering the same object... like on this picture.<br></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 445.52px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 445.52px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5"><br>Measuring latency this way isn&rsquo;t straightforward - we need to identify the event happening when laptop screen refreshes (on 75Hz refresh rate) and find the same event on slow motion video timescale (240Hz rate). Then, we need to find many of those, as </span><span class="c0 c5">&ldquo;</span><span class="c10 c0 c5">camera sample to display&rdquo; period could vary, depending on how much 16.6ms camera (60Hz) sample period overlaps with 13.3 display sync period.</span></p><p class="c2"><span class="c0 c5">In addition, it is important to verify the latency introduced by the marker recognition code, the one that is also measured using </span><span class="c3 c0"><a class="c7" href="#h.fkekl6kcowp">EXT_disjoint_timer_query</a></span><span class="c0 c5">&nbsp;</span><span class="c10 c0 c5">- the same measurement was repeated without recognition code; only displaying captured &nbsp;video.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">This was, too, measured on </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://www.asus.com/us/Laptops/ROG-GL702VM-7th-Gen-Intel-Core/&amp;sa=D&amp;ust=1526980286298000">Asus ROG-GL702VM</a></span><span class="c0 c5">, with discrete graphics, and with </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://support.apple.com/kb/SP704?locale%3Den_US&amp;sa=D&amp;ust=1526980286298000">Mid-2014 MacBook Pro</a></span><span class="c10 c0 c5">, with integrated graphics.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><h1 class="c18" id="h.6u5eurttti0g"><span class="c8 c0">Conclusion</span></h1><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">We can conclude that the result is very promising, at least when it comes to recognition code latency - 0.3 ms. Overall event-to-screen latency, which include camera latency, browser video capture, media decoding and WebGL pipeline and the display latency is measured from 90-120 ms. 90ms is measured with 75Hz </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://www.asus.com/us/Laptops/ROG-GL702VM-7th-Gen-Intel-Core/&amp;sa=D&amp;ust=1526980286299000">Asus ROG-GL702VM</a></span><span class="c0 c5 c10">&nbsp;monitor refresh rate.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">In upcoming work, it is required to verify how this, total latency feels when using webcam and browser for </span><span class="c3 c0"><a class="c7" href="#h.csfaz1ff6j0f">user input</a></span><span class="c10 c0 c5">.</span></p><p class="c2 c11"><span class="c10 c0 c5"></span></p><p class="c2"><span class="c0 c5">This is open source code on </span><span class="c3 c0"><a class="c7" href="https://www.google.com/url?q=https://github.com/intel/depthcamera-3d-model-web-demo&amp;sa=D&amp;ust=1526980286300000">github.com</a></span><span class="c10 c0 c5">&nbsp;- please take a look and contribution is welcome. Thanks.</span></p><hr class="c30"><div><p class="c13"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c19">&nbsp;</span><span class="c0 c5">Ranging from 0.2ms to 6ms, or more, depending on graphics hardware. See </span><span class="c3 c0"><a class="c7" href="#h.fkekl6kcowp">Latency measurement using EXT_disjoint_timer_query</a></span><span class="c0 c5">.</span></p></div><div><p class="c13"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c19">&nbsp;Several depth capture demos and articles are available from </span><span class="c9 c19"><a class="c7" href="https://www.google.com/url?q=https://github.com/01org/depth-camera-web-demo&amp;sa=D&amp;ust=1526980286301000">Depth Camera Web Demo</a></span><span class="c10 c21 c19">&nbsp;project&rsquo;s front page.</span></p></div><div><p class="c13"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c19">&nbsp;Corner labeling from 0 to 3 is further explained in </span><span class="c9 c19"><a class="c7" href="#h.y0xfjuvtvrsn">the algorithm description</a></span><span class="c10 c21 c19">.</span></p></div><div><p class="c13"><a href="#ftnt_ref4" id="ftnt4">[4]</a><span class="c19">&nbsp;</span><span>24 x 18 = 960/40 x 540/30</span></p></div></body></html>