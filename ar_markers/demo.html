<html>
<head>
</head>

<style>
  body {
    display: flex;
    flex-direction: column;
    font-family: 'Roboto', 'Noto', sans-serif;
    line-height: 1.5;
    background-color: #fbfbfb;
    margin: 20px;
  }

  #console {
    color: red;
    font-size: 150%;
    margin: 16px 0px;
  }

  canvas {
    border: 1px solid #cccccc;
    background-color: white;
  }

</style>

<template id="video-stream">
  <style>
    :host {
      display: flex;
      flex-flow: row wrap;
    }

    div {
      margin-right: 16px;
    }

    canvas {
      padding: 16px;
      align-self: center;
      z-index: 1;
    }

    div {
      margin: 16px;
    }

    label {
      display: block;
      margin: 16px 0px;
    }

  </style>
  <div style="position:relative;">
    <canvas id="canvasGL"></canvas>
  </div>
  <div>
    <canvas id="canvas2D" style="visibility:hidden;"></canvas>
  </div>
</template>

<body onload="onLoad()">

  <video-stream></video-stream>

  <div id="console">
    <!-- Print error messages here. -->
  </div>
</body>

<script type="text/javascript" src="script.js"></script>
<script>

  let error = window.console.error;
  window.console.error = (message, ...rest) => {
    let target = document.querySelector('#console');
    error.call(window.console, message, ...rest);

    if (message instanceof Error) {
      message = `${message.name}: ${message.message}`;
    }

    target.innerHTML += `${message}<br>`;
  }
  
  if (window.customElements === undefined) {
    console.error("Code uses customElements and it seems that it is not enabled in your browser.");
  }

  customElements.define('video-stream', class extends HTMLElement {
    constructor() {
      super();
      const template = document.querySelector('#video-stream');
      const clone = document.importNode(template.content, true);

      if (this.attachShadow === undefined) {
        console.error("Code uses Element.attachShadow and it seems that Shadow DOM API is not enabled in your browser.");
      }

      const shadowRoot = this.attachShadow({ mode: 'open' });
      this.shadowRoot.appendChild(clone);

      this._frameLoop = this._frameLoop.bind(this);

      this.readBuffer = null;
      this.readFormat = null;
    }


    connectedCallback() {
      const canvas = this.shadowRoot.getElementById("canvas2D");
      const canvasgl = this.shadowRoot.getElementById("canvasGL");
      this.readFullPixels = canvas.offsetWidth > 0 || canvas.offsetHeight > 0;
      this.ctx2d = canvas.getContext("2d");

      this.frameAvailable = false;

      this.video = this._createOffscreenVideo();
      this.video.oncanplay = _ => {
        canvas.width = this.video.videoWidth;
        canvas.height = this.video.videoHeight;      
        canvasgl.width = this.video.videoWidth;
        canvasgl.height = this.video.videoHeight;                
        this.gl = this._configureGLContext();
        this.frameAvailable = true;
      }
      this.video.addEventListener("play", this._frameLoop);
    }

    _createOffscreenVideo() {
      return Object.assign(document.createElement("video"), {
        autoplay: true,
        loop: true,
        crossOrigin: "anonymous",
        width: 960,
        height: 540
      });
    }

    sleep(ms) {
      return new Promise(resolve => setTimeout(resolve, ms));
    }
    
    async _frameLoop() {
      if (this.frameAvailable) {
        const gl = this.gl;
        gl.activeTexture(gl.TEXTURE0);
        gl.bindTexture(gl.TEXTURE_2D, gl.video_texture);

        // Upload the video frame to texture.
        gl.activeTexture(gl.TEXTURE0);
        gl.bindTexture(gl.TEXTURE_2D, gl.video_texture);
        gl.texImage2D(gl.TEXTURE_2D, 0, gl.RGBA, gl.RGBA, gl.UNSIGNED_BYTE, this.video);

        if (gl.timer_query) {
          // check if previous run results are available:
          if (gl.timer_query_pending &&
              gl.getQueryParameter(gl.timer_query, gl.QUERY_RESULT_AVAILABLE)) {
            const elapsed = gl.getQueryParameter(gl.timer_query, gl.QUERY_RESULT) / 1000000;
            console.log("GPU time elapsed on recognition passes: " + elapsed + "ms.");
            gl.timer_query_pending = false;
          }
          if (!gl.timer_query_pending)
            gl.beginQuery(gl.timer_extension.TIME_ELAPSED_EXT, gl.timer_query);
        }

        for (let i = 0; i < gl.passes.length; ++i) {
          const pass = gl.passes[i];
          // comment previous two lines and uncomment following to measure
          // latency of rendering only
          // { const pass = gl.passes[6];
          gl.useProgram(pass.program);
          gl.bindFramebuffer(gl.FRAMEBUFFER, pass.framebuffer);

          gl.bindBuffer(gl.ARRAY_BUFFER, gl.vertex_buffer);
          gl.vertexAttribPointer(pass.program.vertex_location, 2, gl.FLOAT, false, 0, 0);

          if(pass.outlines) {
            gl.lineWidth(3);
            gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, gl.line_index_buffer);
            gl.drawElementsInstanced(gl.LINE_LOOP, 4, gl.UNSIGNED_SHORT, 0, pass.outlines);
          } else if (pass.codes) {  // codes
            gl.enable(gl.BLEND);
            gl.bindTexture(gl.TEXTURE_2D, gl.codes_texture);
            gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, gl.index_buffer);
            gl.drawElementsInstanced(gl.TRIANGLES, 6, gl.UNSIGNED_SHORT, 0, pass.codes);    
            gl.disable(gl.BLEND);
          } else {
            gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, gl.index_buffer);
            gl.drawElements(gl.TRIANGLES, 6, gl.UNSIGNED_SHORT, 0);              
          }
          if (i == 5 && gl.timer_query && !gl.timer_query_pending) {
            gl.endQuery(gl.timer_extension.TIME_ELAPSED_EXT);
            gl.timer_query_pending = true;
          }
        }

        // Read it back to buffer.
        if (this.readFullPixels) {
          gl.bindFramebuffer(gl.FRAMEBUFFER, gl.passes[2].framebuffer);
          this._readPixels();
          this._putReadPixelsTo2DCanvas();
        }
        // this.readPixels = true;
        if (this.readPixels) {
          const pass = gl.passes[5];
          gl.bindFramebuffer(gl.FRAMEBUFFER, pass.framebuffer);
          this._readFloatPixels(pass.out[0].w, pass.out[0].h);
          // Put read and processed pixels to 2D canvas.
          // Note: This is just one of scenarios for the demo. You can directly
          // bind video to 2D canvas without using WebGL as intermediate step.
          // this._putReadPixelsTo2DCanvas();
          this._putReadFloatPixelsTo2DCanvas(pass.out[0].w, pass.out[0].h);
        }
        // await this.sleep(1000); // used for testing
      }
      window.requestAnimationFrame(this._frameLoop);
    }

    _readPixels() {
      const gl = this.gl;
      this.readBuffer = new Uint8Array(this.video.videoWidth * this.video.videoHeight * 4);
      gl.readPixels(0, 0, this.video.videoWidth, this.video.videoHeight, gl.RGBA, gl.UNSIGNED_BYTE, this.readBuffer);        
    }

    _putReadPixelsTo2DCanvas() {
      const img = this.ctx2d.getImageData(0, 0, this.video.videoWidth, this.video.videoHeight);
      const data = img.data;
      for (let i = 0, length = data.length; i < length; i++) {  
        data[i] = this.readBuffer[i];
      }
      this.ctx2d.putImageData(img, 0, 0);
    }

    _readFloatPixels(w, h) {
      const gl = this.gl;
      this.readBuffer = new Float32Array(w * h * 4);
      gl.readPixels(0, 0, w, h, gl.RGBA, gl.FLOAT, this.readBuffer);        
    }

    _putReadFloatPixelsTo2DCanvas(w, h) {
      const width = this.video.videoWidth;
      const height = this.video.videoHeight;

      for (let j = 0; j < h; j++) {
        const row = j * w * 4;
        const rend = (j + 1) * w * 4;
        for (let i = row; i < rend; i+=4) {
          if (this.readBuffer[i] != 0.0 && this.readBuffer[i+1] != 0.0) {
            // Calculate the pixel.
            let x = (((this.readBuffer[i] % 1.0) * width) | 0) + 3;
            let y = (((this.readBuffer[i+1] % 1.0) * height) | 0) + 3;

            const index = (this.readBuffer[i] | 0) >> 10;
            
            if (!this.readFullPixels)
              continue;


            if (this.readBuffer[i+2] != 0 && this.readBuffer[i+3] != 0) {
              this.ctx2d.beginPath();
              this.ctx2d.fillStyle = "#00FFFF";
              this.ctx2d.fillRect(x, y, 2, 2);
              this.ctx2d.stroke();

              this.ctx2d.beginPath();
              this.ctx2d.strokeStyle = "#FF0000";
              this.ctx2d.moveTo(x, y);

              const x1 = (((this.readBuffer[i+2] % 1.0)* width) | 0) + 3;
              const y1 = (((this.readBuffer[i+3] % 1.0) * height) | 0 + 3);
              this.ctx2d.lineTo(x1, y1);
  
              this.ctx2d.fillStyle = "#FFA500";
              this.ctx2d.fillRect(x1, y1, 2, 2);

              // Now, check if the rest of pixels are available in integer part:
              if (this.readBuffer[i+1] > 1.0 || this.readBuffer[i+2] > 1.0) {
                const x2 = (this.readBuffer[i] | 0) & 0x3FF;
                const y2 = this.readBuffer[i+1] | 0;
                this.ctx2d.lineTo(x2, y2);
                const x3 = this.readBuffer[i+2] | 0;
                const y3 = this.readBuffer[i+3] | 0;
                this.ctx2d.lineTo(x3, y3);
              }
              this.ctx2d.stroke();
            }
          }
        }
      }
    }

    // Creates WebGL/WebGL2 context used to upload depth video to texture,
    // read the pixels to Float buffer and optionElally render the texture.
    _configureGLContext() {
      const width = this.video.videoWidth;
      const height = this.video.videoHeight;
      const canvas = this.shadowRoot.getElementById("canvasGL");
      const gl = canvas.getContext("webgl2",  {antialias: false});
      
      return initGLForARMarkerDetection(gl, width, height);
    }

    async loadStream() {
      if (window.stream) {
        for (let track of window.stream.getTracks()) {
          track.stop();
        }
      }

      const getUserMedia = () => {
        const constraints = {
          video: {
            frameRate: {ideal: 60},
            width: {ideal: this.video.width},
            height: {ideal: this.video.height}
          }
        }
        return navigator.mediaDevices.getUserMedia(constraints);
      }

      try {
        const stream = await getUserMedia();
        this.video.srcObject = stream;
      } catch (err) {
        console.error(err);
      }
    }
  });

  function onLoad() {
    const videoStreamEl = document.querySelector('video-stream');
    videoStreamEl.loadStream();
  }
</script>
</html>